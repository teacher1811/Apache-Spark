package org.example;

import org.apache.spark.sql.*;

public class SparkSQLAdvanced {
    public static void main(String[] args) {
        // Inicializar SparkSession
        SparkSession spark = SparkSession.builder()
                .appName("Spark SQL Advanced Problem")
                .master("local[*]") // Usa todos los nÃºcleos disponibles
                .getOrCreate();

        // ğŸ“Œ Cargar pedidos desde JSON
        Dataset<Row> pedidosDF = spark.read().json("src/main/input/pedidos.json");
        pedidosDF.printSchema();
        pedidosDF.show();

        // ğŸ“Œ Cargar clientes desde CSV
        Dataset<Row> clientesDF = spark.read()
                .option("header", "true")  // Primera fila como encabezado
                .option("inferSchema", "true") // Inferir tipos de datos
                .csv("src/main/input/clientes.csv");

        clientesDF.printSchema();
        clientesDF.show();

        // ğŸ“Œ 1ï¸âƒ£ JOIN entre pedidos y clientes (INNER JOIN por id_cliente)
        Dataset<Row> pedidosClientes = pedidosDF
                .join(clientesDF, "id_cliente"); // INNER JOIN

        pedidosClientes.show();

        // ğŸ“Œ 2ï¸âƒ£ Total de ventas por ciudad
        pedidosClientes.groupBy("ciudad")
                .sum("precio")
                .withColumnRenamed("sum(precio)", "total_ventas")
                .show();

        // ğŸ“Œ 3ï¸âƒ£ Pedidos con precio mayor a 500â‚¬
        pedidosClientes.filter(pedidosClientes.col("precio").gt(500)).show();

        // ğŸ“Œ 4ï¸âƒ£ NÃºmero de compras por cliente (filtrar los que compraron mÃ¡s de 2 veces)
        pedidosClientes.groupBy("id_cliente", "nombre")
                .count()
                .filter("count > 2")
                .withColumnRenamed("count", "total_compras")
                .show();

        // ğŸ“Œ 5ï¸âƒ£ Producto mÃ¡s vendido
        pedidosClientes.groupBy("producto")
                .count()
                .orderBy(functions.desc("count"))
                .limit(1)
                .show();

        // Detener Spark
        spark.stop();
    }
}
